{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import some libraries\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "from torch.autograd import Variable\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert vector to image\n",
    "\n",
    "def to_img(x):\n",
    "    x = 0.5 * (x + 1)\n",
    "    x = x.view(x.size(0), 28, 28)\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Displaying routine\n",
    "\n",
    "def display_images(in_, out, n=1):\n",
    "    for N in range(n):\n",
    "        if in_ is not None:\n",
    "            in_pic = to_img(in_.cpu().data)\n",
    "            plt.figure(figsize=(18, 6))\n",
    "            for i in range(4):\n",
    "                plt.subplot(1,4,i+1)\n",
    "                plt.imshow(in_pic[i+4*N])\n",
    "                plt.axis('off')\n",
    "        out_pic = to_img(out.cpu().data)\n",
    "        plt.figure(figsize=(18, 6))\n",
    "        for i in range(4):\n",
    "            plt.subplot(1,4,i+1)\n",
    "            plt.imshow(out_pic[i+4*N])\n",
    "            plt.axis('off')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define data loading step\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "dataset = FashionMNIST('./data', transform=img_transform, download=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "testset = FashionMNIST('./data/', transform=img_transform)\n",
    "testloader = DataLoader(testset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define model architecture \n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            #1st Layer\n",
    "            nn.Conv2d(in_channels=1,out_channels=16, kernel_size=3, stride=3, padding=1),#output of size 10\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=2, padding=0),#output of size 5\n",
    "            #2nd layer\n",
    "            nn.Conv2d(in_channels=16, out_channels=8, kernel_size=3, stride=2, padding=1), #output of size 3\n",
    "            nn.LeakyReLU(),\n",
    "            nn.AvgPool2d(kernel_size=2, stride=1, padding=0), #output of size 2\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=16, kernel_size=3, stride=2, padding=0),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=16, out_channels=8, kernel_size=5, stride=3, padding=1),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.ConvTranspose2d(in_channels=8, out_channels=1, kernel_size=2, stride=2, padding=1),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        #Since we're interested in classification we can ignore \n",
    "        #the decoding part of the last network and just feed the output of \n",
    "        #the deepest hidden layer into the classifier layer: the output of the encoding part(size 2)\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(8*2*2, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, 200),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(200, 300),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(300, 10),\n",
    "        )\n",
    "        \n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        #x = self.decoder(x)\n",
    "        x = x.view(-1,8*2*2 )\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.log_softmax(x,dim=1)\n",
    "        return x\n",
    "    \n",
    "model = Autoencoder().to(device)\n",
    "criterion = nn.CrossEntropyLoss()   #Since we are doing classification, we use the Cross Entropy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configure the optimiser\n",
    "learning_rate = 1e-3\n",
    "l2_regularizer = 1e-5\n",
    "optimizer = torch.optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=l2_regularizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "loss_list = list()\n",
    "for epoch in range(num_epochs):  # loop over the dataset multiple times\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        # get the inputs\n",
    "        real, labels = data\n",
    "        \n",
    "        inputs = Variable(real)\n",
    "        labels = Variable(labels).long() \n",
    "        \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()             \n",
    "        \n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)           #forward pass\n",
    "        loss = criterion(outputs, labels) # compute the loss\n",
    "        loss.backward()                   #compute the gradients of the loss:backward pass\n",
    "        optimizer.step()                  #update the parameters\n",
    "        \n",
    "        #print statistics\n",
    "    print('epoch [{}/{}], loss:{:.4f}'\n",
    "      .format(epoch+1, num_epochs, loss.data[0]))\n",
    "    loss_list.append(loss.data[0])\n",
    "    #print(loss_list)\n",
    "    #if epoch % 1 == 0:\n",
    "    #    pic = to_img(outputs.data)\n",
    "    #    save_image(pic, './AE_img/{}.png'.format(epoch))\n",
    "\n",
    "#torch.save(model.state_dict(), './conv_autoencoder.pth') \n",
    "print('Finished Training')\n",
    "\n",
    "\n",
    "# Plot of the loss of training\n",
    "nb_epoch = range(1, len(loss_list) + 1)\n",
    "plt.plot(nb_epoch, loss_list)\n",
    "plt.xlabel('Epochs ',fontsize=9)\n",
    "plt.ylabel('Loss',fontsize=9)\n",
    "plt.title('Plot of the Training Loss',fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Test of our network with a test set\n",
    "correct = 0    #count of the number of correct predictions\n",
    "total = 0      #count the total number of predictions\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = Variable(images)  #get the images in the mini-batch\n",
    "        labels = Variable(labels)  #get the labels of images in the mini-batch\n",
    "        outputs = model(images)    #do prediction of each image in the mini-batch\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += float((predicted == labels).sum().item())\n",
    "\n",
    "print('Accuracy of the network: %f %%' % (\n",
    "    100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Lets print out the accuracy of the network on each class\n",
    "class_correct = list(0. for i in range(10))\n",
    "class_total = list(0. for i in range(10))\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        images, labels = data\n",
    "        images = Variable(images)\n",
    "        labels = Variable(labels)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        c = (predicted == labels).squeeze()\n",
    "        for i in range(4):\n",
    "            label = labels[i]\n",
    "            class_correct[label] += c[i].item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    print('Accuracy of classe %5s : %2f %%' % (\n",
    "        [i+1], 100 * class_correct[i] / class_total[i]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
